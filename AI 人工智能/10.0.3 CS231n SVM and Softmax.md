# CS231n SVM and Softmax

> Keywords:
>> **parameters approach**, **score function**, **loss function**, **bias trick**, **hinge loss v.s. cross-entropy loss**, **L2 regulariztion**

* **score function** maps the raw data to class scores
* **hinge loss** max(0, ?)
* **squared hinge loss* $max(0, ?)^2$
* **loss function / cost function / objective** quantifies the agreement between the predicted scores and the ground true labels.
  * Multiclass SVM (Support Vector Machine) lose: is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$.
* **Softmax Classifier**

> **In practice, SVM and Softmax are usually comparable.** The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.

![SVM v.s. Softmax](http://cs231n.github.io/assets/svmvssoftmax.png)

[Linear Classfication Loss Visualization](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)

## Linear classifier

Assume a training dataset of N images $x_i \in R^D, i \in [1,N]$ (each with a dimensionality D. e.g. a 32*32 RGB image, $D = 32*32*3 = 3072$), each assoicated with a label $y_i, y_i \in [1,K]$ (K categories).

$$f(x_i, W, b) = Wx_i + b$$

* **W** a matrix with size of [K * D], parameters in W are often called the **weights**
* **b** the bias vector with size of [K * 1]

e.g. In CS231n, K = 10, D = 32*32*3 = 3072, atrix W's size is [10 \* 3072], bias vector's size is [10 \* 1].

![f(x,W,b)=W+b](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-weight.jpg?raw=true)

## Multiclass SVM Loss

> The loss function quantifies our unhappiness with predictions on the training set.

$$ L_i = \sum_{j{\neq}y_i}max(0, s_j - s_{y_i} + \Delta)$$

* $s_j = f(x_i, W)_j$ score for the j-th class
* $s_{y_i}$ score for the ground-true class
* $max(0,s_j - s_{y_i} + 1)$ is **hinge loss** function
* $\Delta$ can safely be set to 1.0 in all cases

> Q: What if the sum was instead over all classes? (including j=$y_j$)
>
> Q: What if we used a mean instead of a sum?

$$L =  \frac{1}{N}\sum^N_{i=1}L_i$$

L1 is also called the **data loss**

![Multiclass SVM Loss](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-multiclass-svm-loss.jpg?raw=true)

```python
def L_i(x, y, W):
  """
  unvectorized version. Compute the multiclass svm loss for a single example (x,y)
  - x is a column vector representing an image (e.g. 3073 x 1 in CIFAR-10)
    with an appended bias dimension in the 3073-rd position (i.e. bias trick)
  - y is an integer giving index of correct class (e.g. between 0 and 9 in CIFAR-10)
  - W is the weight matrix (e.g. 10 x 3073 in CIFAR-10)
  """
  delta = 1.0 # see notes about delta later in this section
  scores = W.dot(x) # scores becomes of size 10 x 1, the scores for each class
  correct_class_score = scores[y]
  D = W.shape[0] # number of classes, e.g. 10
  loss_i = 0.0
  for j in xrange(D)
    if j == y:
      continue
    loss_i += max(0, scores[j] - correct_class_score + delta)
  return loss_i

def L_i_vectorized(x, y, W):
  """
  A faster half-vectorized implementation. half-vectorized
  refers to the fact that for a single example the implementation contains
  no for loops, but there is still one loop over the examples (outside this function)
  """
  delta = 1.0
  scores = W.dot(x)
  # compute the margins for all classes in one vector operation
  margins = np.maximum(0, scores - scores[y] + delta)
  # on y-th position scores[y] - scores[y] canceled and gave delta. We want
  # to ignore the y-th position and only consider margin on max wrong class
  margins[y] = 0
  loss_i = np.sum(margins)
  return loss_i
```

### L2-SVM Resulartion Penalty R(W)

There is one bug with the loss function we presented above. Suppose that we have a dataset and a set of parameters **W** that correctly classify every example (i.e. all scores are so that all the margins are met, and Li=0 for all i). The issue is that this set of **W** is not necessarily unique: there might be many similar W that correctly classify the examples. One easy way to see this is that if some parameters **W** correctly classify all examples (so loss is zero for each example), then any multiple of these parameters λW where λ>1 will also give zero loss because this transformation uniformly stretches all score magnitudes and hence also their absolute differences. For example, if the difference in scores between a correct class and a nearest incorrect class was 15, then multiplying all elements of W by 2 would make the new difference 30.

In other words, we wish to encode some preference for a certain set of weights W over others to remove this ambiguity. We can do so by extending the loss function with a **regularization penalty R(W)**. The most common regularization penalty is the **L2** norm that discourages large weights through an elementwise quadratic penalty over all parameters:

$$R(W) = \sum_k\sum_{l}W_{k,l}^2$$

 Including the regularization penalty completes the full Multiclass Support Vector Machine loss, which is made up of two components: the **data loss** (which is the average loss Li over all examples) and the **regularization loss**. That is, the full Multiclass SVM loss becomes:

$$L = L1 + L_{rloss} = \frac{1}{N}\sum_{i}L_{i} + {\lambda}R(W)$$

Or expanding this out in its full form:
$$L = L1 + L_{rloss} = \frac{1}{N}\sum_{i}\sum_{j{\ne}y}[max(0, f(x_i; W)_j - f(x_i;W)_{y_i} + \Delta)] + \lambda\sum_k\sum_{l}W_{k,l}^2$$

> L2-SVM penalizes violated margins more strongly.
> All we have to do now is to come up with a way to find the weights that minimize the loss.

### Pracical Considerations

**Setting Delta $\Delta$** It turns out that this hyperparameter can safely be set to $\Delta$=1.0 in all cases. The hyperparameters $\Delta$ and $\lambda$ seem like two different hyperparameters, but in fact they both control the same tradeoff: The tradeoff between the data loss and the regularization loss in the objective. The key to understanding this is that the magnitude of the weights W has direct effect on the scores (and hence also their differences): **As we shrink all values inside W the score differences will become lower, and as we scale up the weights the score differences will all become higher.** Therefore, the exact value of the margin between the scores (e.g. $\Delta$=1, or $\Delta$=100) is in some sense meaningless because the weights can shrink or stretch the differences arbitrarily. Hence, the only real tradeoff is how large we allow the weights to grow (through the regularization strength $lambda$).

## Softmax Classifier

> Two commonly used losses for linear classifier: **SVM** and **Softmax classifier**.

In the Softmax classifier, the function mapping $f(xi;W)=W_xi$ stays unchanged, but we now interpret these scores as the unnormalized log probabilities for each class and replace the **hinge loss** with a **cross-entropy loss** that has the form:

$L_i = -log(\frac{e^{f_{y_i}}}{\sum_je^f_j})$ or equaivalently $L_i = -f^{y_i} + log\sum_je^{f_j}$

where we are using the notation $f_j$ to mean the j-th element of the vector of class scores f.

## SVM v.s Softmax

![Softmax Classifier](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-softmax.jpg?raw=true)

![SVM v.s. Softmax](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-softmax-vs-svm.png?raw=true)

> **Softmax classifier provides “probabilities” for each class.**
>> Unlike the SVM which computes uncalibrated and not easy to interpret scores for all classes, the Softmax classifier allows us to compute *probabilities* for all labels.
> **SVM and Softmax are usually comparable**
>> The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.
