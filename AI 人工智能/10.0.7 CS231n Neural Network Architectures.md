# Neural Network Architectures

## Layer-wise Organization

**Fully-connected layer** in which neurons between two adjacent layers are fully pairwise connected, but neurons within a single layer share no connections.

![Fully Connected Neural Network](https://github.com/AarioAi/Note/blob/master/AI%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/_asset/CS231n-fully-connected-neural-network.jpg?raw=true)

> Naming conventions:
    >> Notice that when we say N-layer neural network, we do not count the input layer. Therefore, a single-layer neural network describes a network with no hidden layers (input directly mapped to output). In that sense, you can sometimes hear people say that logistic regression or SVMs are simply a special case of single-layer Neural Networks. You may also hear these networks interchangeably referred to as “Artificial Neural Networks” (ANN) or “Multi-Layer Perceptrons” (MLP). Many people do not like the analogies between Neural Networks and real brains and prefer to refer to neurons as units.
>
> Sizing neural networks:
    >> The two metrics that people commonly use to measure the size of neural networks are the number of neurons, or more commonly the number of parameters. Working with the two example networks in the above picture: (1) The first network has $4+2=6$ neurons, $(3*4)+(4*2)=20$ weights and $4+2=6$ biases, for a total of 26 learnable parameters. (2) The second network has $4+4+1=9$ neurons, $(3*4)+(4*4)+(4*1)=32$ weights and $4+4+1=9$ biases, for a total of 41 learnable parameters.
    >>
    >> Modern convolutional networks contain on orders of 100 million parameters and are usually made up of approximately 10 to 20 layers (hence deep-learning).

![Fully Connected Layer v.s. Convolutional Layer](https://github.com/AarioAi/Note/blob/master/AI%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/_asset/CS231n-fully-connected-vs-convolutional-layers.png?raw=true)

![CNN Layers](https://github.com/AarioAi/Note/blob/master/AI%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/_asset/CS231n-CNN-layers.jpg?raw=true)

## Feed-forward Neural Network

A feed-forward neural network is an artificial neural network wherein connections between the units do not form a cycle. The feed-forward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There're no cycles or loops in the network.

Repeated matrix multiplications interwoven with activation function. One of the primary reasons that neural networks are organized into layers is that this structure makes it very simple and efficient to evaluate neural networks using matrix vector operations.

Working with the example 3-layer neural network in the diagram above, the input would be a $[3*1]$ vector. *All connection strengths for a layer can be stored in a single matrix*. The first hidden layer's weights $W1$ would be of size $[4*3]$, and the bias $b1$ of size $[4*1]$. The matrix vector multiplication `numpy.dot(W1, x)` evaluates the activations of all neurons in that layer. Similarly, $W2$ would be a $[4*4]$ matrix and $W3$ a $[1*4]$ matrix.

> W1, W2, W3, b1, b2, b3 are the learnable parameters.

```python
# forward-pass of a 3-layer neural network:
f = lambda x: 1.0/(1.0+np.exp(-x))  # logistic sigmoid activation function
x = numpy.random.randn(3, 1)    # random inputs, a 3*1 matrix
hidden_layer1 = f(numpy.dot(W1, x) + b1)
hidden_layer2 = f(numpy.dot(W2, hidden_layer1) + b2)
output = numpy.dot(W3, hidden_layer2) + b3
```

Notice also that instead of having a single input column vector, the variable $x$ could hold an entire batch of training data (where each input example would be a column of $x$) and the all examples would be efficiently evaluated in parallel. Notice that the final neural network layer usually doesn't have an activation function (e.g. it represents a real-valued class score in a classification setting).

> The forward pass of a fully-connected layer corresponds to one matrix multiplication followed by a bias offset and an activation function.

### Representational Power

One way to look at neural networks with fully-connected layers is that they define a family of functions that are parameterized by the weights of the network. A natural question that arises is: *What is the representational power of this family of functions? In particular, are there functions that cannot be modeled with a neural network?*

It turns out that neural networks with at least one hidden layer are **universal approximators**. That is, it can be shown that given any continuous function $f(x)$ and some $\epsilon > 0$, there exists a neural network $g(x)$ with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that $\forall, | f(x)-g(x)s | < \epsilon$. In other words, **the neural network can approximately any continuous function**.

*If on hidden layer suffices to approximate any function, why use more layers and go deeper?* The answer is that the fact that a two-layer neural network is a universal approximator is, while mathematically cute, a relatively weak and useless statement in practice. In one dimension, the "sum of indicator bumps" function $g(x) = {\sum_i}{c_i}𝟙(a_i < x < b_i)$ where $a$,$b$,$c$ are parameter vectors is also a universal approximator, but noone would suggest that we use this functional form in machine learning. Neural networks work well in practice because they compactly express nice, smooth functions that fit well with the statistical properties of data we encounter in practice, and are also easy to learn using our optimization algorithms (e.g. gradient descent). Similarly, the fact that deeper networks can work better than a single-hidden-layer networks is an empirical observation, despite the fact that their representational power is equal.

As an aside, in practice it is often the case that 3-layer neural networks will outperform 2-layer nets, but going even deeper rarely helps much more. This is in stack contrast to convolutional networks, where depth has been found to be an extremely important component for a good recognization system. *One argument for this observation is that images contain hierarchical structure (e.g. faces are made up of eyes, which are made up of edges, etc.), so several layers of processing make intuitive sense for this data domain.*