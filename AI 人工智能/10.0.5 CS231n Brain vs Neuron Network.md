# CS231n Brain v.s. Neuron Network

In the section on linear classification we computed scores for different visual categories given the image using the formula $s=Wx$, where $W$ was a matrix and $x$ was an input column vector containing all pixel data of the image. In the case of CIFAR-10, $x$ is a 3072x1 column vector, and $W$ is a 10x3072 matrix, so that the output scores is a vector of 10 class scores.

## Biological Motivation and Connections

Each neuron receives input signals from its **dendrites** and produces output signals along its **axon**. The axon eventually branches out and connects via **synapses** to dendrites of other neurons. In the computational model of a neuron, the signals that travel along the axons (e.g. $x_0$) interact multiplicatively (e.g. ${w_0}{x_0}$) with the dendrites of the other neuron based on the synaptic strength at that synapses (e.g. $w_0$). The idea is that synaptic strengths (the weighs $w$) are learnable and control the strength of influence (and its direction: excitory (positive weight) or inhibitory (nagative weight)) of one neuron on another. In the basic model, the dendrites carry the signal to the cell body where they all get summed. If the final sum is above a certain threshold, the neuron can *fire*, sending a spike along its axon. In the computational mode, we assume that the precise timings of the spikes do not matter, and taht only the frequency of the firing communicates information. Based on this rate code interpretation, we model the *firing rate* of the neuron with an **activation function** $f$, which represents the frequency of the spikes along the axon. Historically, a common choice of activation function is the **sigmoid function** $\sigma$, since it takes a realy-valued input and squashes it to range between 0 and 1.

![Biological neuron v.s. mathematical model](https://github.com/AarioAi/Note/blob/master/AI%20%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/_asset/CS231n-neuron-vs-math-model.jpg?raw=true)

An example code for forward-propagating a single neuron might look as follows:

```python
class Neuron(object):
    def forward(self, inputs):
        # assume inputs and weights are 1D numpy arrays and bias is a number
        cell_body_sum = np.sum(inputs * self.weights) + self.bias
        firing_rate = 1.0 / (1.0 + math.exp(-cell_body_sum))  # sigmoid activation function
        return firing_rate
```

In other words, each neuron performs a dot product with the input and its weights, adds the bias and applies the non-linearity ( or activation function), in this case the sigmoid $\sigma(x) = \frac{1}{1 + e^{-x}}$.

## Single Neuron as a Linear Classifier

A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers).

**Binary Softmax Classifier** For example, we can interpret $\sigma({\sum}_iw_ix_i + b)$ to be the probability of one of the classes $P(y_i=1 | x_i; w)$. The probability of the other class would be $P(y_i=0 | x_i; w) = 1 - P(y_i=1 | x_i; w)$, since they must sum to be one.
