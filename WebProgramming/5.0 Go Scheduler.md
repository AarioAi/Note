# Scheduling in Go

## OS Scheduler

 The OS scheduler is a preemptive scheduler. Essentially that means you can’t predict what the scheduler is going to do at any given time. The kernel is making decisions and everything is non-deterministic.

Your program is just a series of machine instructions that need to be executed one after the other sequentially. To make that happen, the operating system uses the concept of a Thread. It’s the job of the Thread to account for and sequentially execute the set of instructions it’s assigned. Execution continues until there are no more instructions for the Thread to execute. This is why I call a Thread, “a path of execution”.

Every program you run creates a Process and each Process is given an initial Thread. Threads have the ability to create more Threads. Threads can run concurrently (each taking a turn on an individual core), or in parallel (each running at the same time on different cores). Threads also maintain their own state to allow for the safe, local, and independent execution of their instructions.

The OS scheduler is responsible for making sure cores are not idle if there are Threads that can be executing.

### Executing Instructions

The program counter (**PC**), which is sometimes called the instruction pointer (**IP**), is what allows the Thread to keep track of the next instruction to execute. In most processors, the PC points to the next instruction and not the current instruction.

![The Instruction Pointer](https://github.com/luexu/Note/blob/master/_asset/web/go-schduler-instruction-pointer.jpg?raw=true)

> Remember: the PC is the next instruction, not the current one.

### Thread States

A Thread can be in one of three states:

* **Waiting** the thread may be waiting for hardware (disk, network), the operating system (system calls) or synchronization calls (atomic, mutexes). *These types of latencies are a root cause for bad performance.*
* **Runnable** the thread wants time on a core.
* **Executing**

### Types of Work

* **CPU-Bound** This is work that never creates a situation where the Thread may be placed in Waiting state.
* **IO-Bound** This is work that causes Treads to enter into **Waiting** states.

### Context Switching

Thread priorities together with events, (like receiving data on the network) make it impossible to determine what the scheduler will choose to do and when. **The physical act of swapping Threads on a core is called a context switch.** A context switch happens when the scheduler pulls an Executing thread off a core and replaces it with a Runnable Thread. The Thread that was selected from the run queue moves into an Executing state. The Thread that was pulled can move back into a Runnable state (if it still has the ability to run), or into a Waiting state (if was replaced because of an IO-Bound type of request).

Context switches are considered to be expensive because it takes times to swap Threads on and off a core. *The amount of latency incurrent during a context switch depends on different factors but it’s not unreasonable for it to take between ~1000 and ~1500 nanoseconds. Considering the hardware should be able to reasonably execute (on average) 12 instructions per nanosecond per core, a context switch can cost you ~12k to ~18k instructions of latency. In essence, your program is losing the ability to execute a large number of instructions during a context switch.*

If you have a program that is focused on IO-Bound work, then context switches are going to be an advantage. If your program is focused on CPU-Bound work, then context switches are going to be a performance nightmare.

Less Threads in a Runnable state means less scheduling overhead and more time each Thread gets over time. More Threads in a Runnable state mean less time each Thread gets over time. That means less of your work is getting done over time as well.

There is a balance you need to find between the number of cores you have and the number of Threads you need to get the best throughput for your application. When it comes to managing this balance, Thread pools were a great answer.

### Cache Lines

Accessing data from main memory has such a high latency cost (~100 to ~300 clock cycles) that processors and cores have local caches to keep data close to the hardware threads that need it. Accessing data from caches have a much lower cost (~3 to ~40 clock cycles) depending on the cache being accessed. 

![Core i7-9xx Cache Hierarchy](https://github.com/luexu/Note/blob/master/_asset/web/go-schduler-inter-i7-cache-line.png?raw=true)

Data is exchanged between the processor and main memory using cache lines. A cache line is a 64-byte chunk of memory that is exchanged between main memory and the caching system. Each core is given its own copy of any cache line it needs, which means the hardware uses value semantics. This is why mutations to memory in multithreaded applications can create performance nightmares.

## Go Scheduler

When your Go program starts up, it’s given a Logical Processor (P) for every virtual core that is identified on the host machine. If you have a processor with multiple hardware threads per physical core (Hyper-Threading), each hardware thread will be presented to your Go program as a virtual core. To better understand this, take a look at the system report for my MacBook Pro.

```txt
Hardware Overview:
  Model Name:               MacBook Pro
  Model Identifier:         MacBookPro14,3
  Processor Name:           Intel Core i7
  Processor Speed:          2.8 GHz
  Number of Processors:     1
  Total Number of Cores:    4
  L2 Cache (per Core):      256 KB
  L3 Cache:                 6 MB
  Memory:                   16 GB
```

 The Intel Core i7 processor has Hyper-Threading, which means there are 2 hardware threads per physical core. This will report to the Go program that 8 virtual cores are available for executing OS Threads in parallel.

```go
fmt.Println(runtime.NumCPU())       // the number of logical CPUs. It'll be 8 on above MacBook Pro
```

Every P is assigned an OS Thread (“M”). The ‘M’ stands for machine. This Thread is still managed by the OS and the OS is still responsible for placing the Thread on a Core for execution. This means when I run a Go program on my machine, I have 8 threads available to execute my work, each individually attached to a P.

Every Go program is also given an initial Goroutine (“G”), which is the path of execution for a Go program.  Just as OS Threads are context-switched on and off a core, Goroutines are context-switched on and off an M.

The last piece of the puzzle is the run queues. There are two different run queues in the Go scheduler: the Global Run Queue (GRQ) and the Local Run Queue (LRQ). Each P is given a LRQ that manages the Goroutines assigned to be executed within the context of a P. These Goroutines take turns being context-switched on and off the M assigned to that P. The GRQ is for Goroutines that have not been assigned to a P yet. There is a process to move Goroutines from the GRQ to a LRQ that we will discuss later.

![GRQ LRQ](https://github.com/luexu/Note/blob/master/_asset/web/go-schduler-GRQ-LRQ.png?raw=true)

The Go scheduler is part of the Go runtime, and the Go runtime is built into your application. **This means the Go scheduler runs in user space, above the kernel.** The current implementation of the Go scheduler is not a preemptive scheduler but a cooperating scheduler. Being a cooperating scheduler means the scheduler needs well-defined user space events that happen at safe points in the code to make scheduling decisions.

### Goroutine States

* **Waiting**
* **Runnable** the goroutine wants time on an M
* **Executing**

### Context Switcing

## References

* https://www.ardanlabs.com/blog/2018/08/scheduling-in-go-part1.html