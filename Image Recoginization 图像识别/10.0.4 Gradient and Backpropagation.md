# Gradient Descent


## Gradient Descent and Learning Rate

$$f(x) = kx + b \rightarrow  k = \frac{{\delta}f(x)}{{\delta}x} = lim_{h->0}\frac{f(x+h) - f(x)}{h}$$

$$f(x,y)=xy  \rightarrow  \frac{{\delta}f}{{\delta}x} = y,  \frac{{\delta}f}{{\delta}x} = x$$

$$ gradient: \frac{d(fx)}{dx} = lim_{h->0}\frac{f(x+h)-f(x)}{h}$$

![Gradient Demo](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-gradient-demo.jpg?raw=true)

$$f(x = -2, y = 5, z = -4) = (x+y) * z$$

$$q=x+y = 1x + y = 1y + x \rightarrow \frac{{\delta}q}{{\delta}x} =  \frac{{\delta}q}{{\delta}y} = 1$$

$$f=qz \rightarrow \frac{{\delta}f}{{\delta}q} = z; \frac{{\delta}f}{{\delta}z} = q$$

The **chain rule** tells us that the correct way to “chain” these gradient expressions together is through multiplication. E.g., $\frac{{\delta}f}{{\delta}x} = \frac{{\delta}f}{{\delta}q} * \frac{{\delta}q}{{\delta}x}$

At the end we are left with the gradient in the variables [$\frac{{\delta}f}{{\delta}x}$, $\frac{{\delta}f}{{\delta}y}$, $\frac{{\delta}f}{{\delta}z}$], which tell us the sensitivity of the variables x,y,z on f!. This is the simplest example of **backpropagation**.

> $\frac{{\delta}f}{{\delta}x} = -4$ if x were to descrease (responding to their negative gradient) then the add gate's output q would decrease, which inturn makes the multiply gate's output f increase.
>> Backpropagation can thus be thought of as gates communicating to each other (through the gradient signal) whether they want their outputs to increase or decrease (and how strongly), so as to make the final output value higher.

### Intuitive understanding of backpropagation

Every gate in a circuit diagram gets some inputs and can right away compute two things: 1. its output value and 2. the local gradient of its inputs with respect to its output value. Chain rule says that the gate should take that gradient and multiply it into every gradient it normally computes for all of its inputs.

> This extra multiplication (for each input) due to the chain rule can turn a single and relatively useless gate into a cog in a complex circuit such as an entire neural network.

![Gradient](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-gradient.jpg?raw=true)

* numerical gradient: approximate, slow, easy to write
* analytic gradient: exact, fast, error-prone
  
> **Gradient Check**: in practice,  always use analystic gradient, but check implementation with numerical gradient.
> Only use a small portion of the training set to compute the gradient. Common mini-batch sizes are 32/64/128/256 examples.

```python
while True:
    data_batch = sample_training_data(data, 256)    # sample 256 examples
    weights_grad = evaluate_gradient(loss_fun, data, weights)
    weights += -learning_rate * weight_grad    #  learning_rate is also called step_size
```

![The effects of learning rate](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-the-effects-of-learning-rate.jpg?raw=true)

## Reference
* http://cs231n.github.io/linear-classify/
* http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/