# Image Classification Datasets

The most difficult part of creating a dataset is labeling the data. This laborious process significatly slows down the creating of a dataset. 

To make labeling easier, some existing datasets use images or vidoes in which the objects of interest stand out. E.g. creating lables by using **real-time geo-tagged** visual data.

> **Instance segmentation labels** take more time to annotate than
**bounding-boxes labels**.

## Visual Datasets

http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354

* MNIST
* CIFAR-10  玩具级别
  * 10 classes, 60,000 32*32 images
    * 50,000 images training set
    * 10,000 images test set
  * http://www.cs.toronto.edu/~kriz/cifar.html
* CIFAR-100
* STL-10
* SVHN
* ILSVRC2012 task 1
* Pascal VOC (Visual Object Classification)
  * collect images from Flick
  * 2005~2011 (20 categories, 10,000 images for training and validation containing **bounding boxes** labels)
  * 2012 (20 classes, 27,450 detection objects in 11,530 images for training and validation containing **bouding boxes** labels)
  * 2013 (200 categories, 500,000 images only for training)
* ImageNet
  * used as the training data for AlexNet and the LPIRC
  * 200 classes, 465,567 training images, 20,121 validation images
* SUN (Scene UNderstanding)
  * 4,479 classes, 313,884 instance segmentation labels in 131,067 images containing **instance segmentation** labels.
  * 6,202 instances of people in 2,062 images
* INRIA People Dataset
  * 1 class(people), 1,237 **bounding-box** labels in 614 positive images (people are labeled in the image), 1,218 negative images containing no labels.
  * Despite the missing labels, the original INRIA dataset is still popular and has made laudable contributions to pedestrian detection.
* KITTI
  * contains a variety of labels for tracking, scene flow, odometry, etc
* Caltech Pedestrain Dataset
  * consists of approximately ten hours of 600 × 400 taken at 30 frames per second video from a vehicle driving through regular urban traffic.
  * This is different from PASCAL VOC’s handling of occluded images, where only the visible portion of an object is marked.
* Microsoft COCO (Common Object in Context)
  * collect images from Flick
  * Common refers to the objects that can be *easily recognizeable by a 4-year old*
  * 2.5 million labeled instances in 382,000 images containing **instance segmentations**

> Potential dataset issues: (1) the image selection bias (e.g. Pascal VOC, COCO, all collect images from FLickr); (2) labeling quality.
>> Network camera solve these two issues: For (1), its data is a completely new repository for datasets; For (2), network cameras could cross-referenced with events (such as weather) to automatically lable the images for classification. *While this does not yet solve the missing labels for object detection*.

## Automatic Labeling Systems

### CAM2

The CAM2 system provides users realtime data analysis tools which are run using the CAM2’s Cloud Computing. Some of the current tools provided by CAM2 are edge detection, motion detection, and color quantization.

Two known issues with an automatic labeling system for the CAM2 system. 1. Change camera's viewpoint, there may be categories marked as present for the camera which are not acturally present in the current viewpoint. 2. the redundancy in data pulled from the same camera (i.g., the background is the same)



## References

* Comparison of Visual Datasets for Machine Learning
  * https://ecommons.luc.edu/cgi/viewcontent.cgi?article=1148&context=cs_facpubs