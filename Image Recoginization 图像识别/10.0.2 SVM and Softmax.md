# SVM and Softmax

> Keywords:
>> **parameters approach**, **score function**, **loss function**, **bias trick**, **hinge loss**, **cross-entropy loss**, **L2 regulariztion**

* **score function** maps the raw data to class scores
* **loss function / cost function / objective** quantifies the agreement between the predicted scores and the ground true labels.
  * Multiclass SVM (Support Vector Machine) lose: is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$.
* **Softmax Classifier**

> **In practice, SVM and Softmax are usually comparable.** The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.

![SVM v.s. Softmax](http://cs231n.github.io/assets/svmvssoftmax.png)

[Linear Classfication Loss Visualization](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)

## Linear classifier

Assume a training dataset of N images $x_i \in R^D, i \in [1,N]$ (each with a dimensionality D. e.g. a 32*32 RGB image, $D = 32*32*3 = 3072$), each assoicated with a label $y_i, y_i \in [1,K]$ (K categories).

$$f(x_i, W, b) = Wx_i + b$$

* **W** a matrix with size of [K * D], parameters in W are often called the **weights**
* **b** the bias vector with size of [K * 1]

e.g. In CS231N, K = 10, D = 32*32*3 = 3072, atrix W's size is [10 \* 3072], bias vector's size is [10 \* 1].

## Multiclass SVM Loss

> The loss function quantifies our unhappiness with predictions on the training set.

$$ L_i = \sum_{j{\neq}y_i}max(0, s_j - s_{y_i} + \Delta)$$

* $s_j = f(x_i, W)_j$ score for the j-th class
* $s_{y_i}$ score for the ground-true class
* $max(0,-)$ is **hinge loss** function

> L2-SVM uses $max(0, -)^2$ that penalizes violated margins more strongly.

![SVM Loss](http://cs231n.github.io/assets/margin.jpg)

## Softmax Classifier

> Two commonly used losses for linear classifier: **SVM** and **Softmax classifier**.

$L_i = -log(\frac{e^{f_{y_i}}}{\sum_je^f_j})$ or equaivalently $L_i = -f^{y_i} + log\sum_je^{f_j}$

## SVM v.s Softmax

![SVM v.s. Softmax](http://cs231n.github.io/assets/svmvssoftmax.png)

> **Softmax classifier provides “probabilities” for each class.**
>> Unlike the SVM which computes uncalibrated and not easy to interpret scores for all classes, the Softmax classifier allows us to compute *probabilities* for all labels.
> **SVM and Softmax are usually comparable**
>> The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.

## Reference
* http://cs231n.github.io/linear-classify/
* http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/