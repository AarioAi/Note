# SVM and Softmax

> Keywords:
>> **parameters approach**, **score function**, **loss function**, **bias trick**, **hinge loss**, **cross-entropy loss**, **L2 regulariztion**

* **score function** maps the raw data to class scores
* **loss function / cost function / objective** quantifies the agreement between the predicted scores and the ground true labels.
  * Multiclass SVM (Support Vector Machine) lose: is set up so that the SVM “wants” the correct class for each image to a have a score higher than the incorrect classes by some fixed margin $\Delta$.
* **Softmax Classifier**

> **In practice, SVM and Softmax are usually comparable.** The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.

![SVM v.s. Softmax](http://cs231n.github.io/assets/svmvssoftmax.png)

[Linear Classfication Loss Visualization](http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/)

## Linear classifier

Assume a training dataset of N images $x_i \in R^D, i \in [1,N]$ (each with a dimensionality D. e.g. a 32*32 RGB image, $D = 32*32*3 = 3072$), each assoicated with a label $y_i, y_i \in [1,K]$ (K categories).

$$f(x_i, W, b) = Wx_i + b$$

* **W** a matrix with size of [K * D], parameters in W are often called the **weights**
* **b** the bias vector with size of [K * 1]

e.g. In CS231n, K = 10, D = 32*32*3 = 3072, atrix W's size is [10 \* 3072], bias vector's size is [10 \* 1].

![f(x,W,b)=W+b](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-weight.jpg?raw=true)

## Multiclass SVM Loss

> The loss function quantifies our unhappiness with predictions on the training set.

$$ L_i = \sum_{j{\neq}y_i}max(0, s_j - s_{y_i} + \Delta)$$

* $s_j = f(x_i, W)_j$ score for the j-th class
* $s_{y_i}$ score for the ground-true class
* $max(0,s_j - s_{y_i} + 1)$ is **hinge loss** function

> Q: What if the sum was instead over all classes? (including j=$y_j$)
>
> Q: What if we used a mean instead of a sum?

$$L = \frac{1}{N}\sum^N_{i=1}L_i$$

![Multiclass SVM Loss](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-multiclass-svm-loss.jpg?raw=true)

> L2-SVM penalizes violated margins more strongly.

## Softmax Classifier

> Two commonly used losses for linear classifier: **SVM** and **Softmax classifier**.

$L_i = -log(\frac{e^{f_{y_i}}}{\sum_je^f_j})$ or equaivalently $L_i = -f^{y_i} + log\sum_je^{f_j}$

## SVM v.s Softmax

![Softmax Classifier](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-softmax.jpg?raw=true)

![SVM v.s. Softmax](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-softmax-vs-svm.png?raw=true)

> **Softmax classifier provides “probabilities” for each class.**
>> Unlike the SVM which computes uncalibrated and not easy to interpret scores for all classes, the Softmax classifier allows us to compute *probabilities* for all labels.
> **SVM and Softmax are usually comparable**
>> The performance difference between the SVM and Softmax are usually very small, and different people will have different opinions on which classifier works better.

## Learning Rate

$$ gradient: \frac{d(fx)}{dx} = lim_{h->0}\frac{f(x+h)-f(x)}{h}$$

![Gradient](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-gradient.jpg?raw=true)

* numerical gradient: approximate, slow, easy to write
* analytic gradient: exact, fast, error-prone
  
> **Gradient Check**: in practice,  always use analystic gradient, but check implementation with numerical gradient.
> Only use a small portion of the training set to compute the gradient. Common mini-batch sizes are 32/64/128/256 examples.

```python
while True:
    data_batch = sample_training_data(data, 256)    # sample 256 examples
    weights_grad = evaluate_graditent(loss_fun, data, weights)
    weights += - learning_rate * weight_grad    #  learning_rate is also called step_size
```

![The effects of learning rate](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-the-effects-of-learning-rate.jpg?raw=true)

## Reference
* http://cs231n.github.io/linear-classify/
* http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/