# Activation Functions

It's also known as **Transfer Function**. It's used to determine the output of neural network like yes or no.

![Activation Functions Cheetsheet](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-activation-functions.png?raw=true)

![Derivative of Activation Functions](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-derivative-of-activation-functions.png?raw=true)

> "What neuron type should I use?" Use the ReLU non-linearity, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network. If this concerns you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.

## Sigmoid or Logistic Activation Function

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

![Sigmoid and Tanh](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-activation-functions.jpg?raw=true)

The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). *Since probability of anything exists only between the range 0 and 1, sigmoid is the right choice.* The **softmax function** is a more generalized logistic activation function which is used for multiclass classification.

The function is differentiable. That means, we can find the slope of the sigmoid curve at any two points.

> When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope. CSThat is why we use differentiation in almost every part of Machine Learning and Deep Learning.

The function is monotonic but function's derivative is not.

In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:

* *Sigmoids saturate and kill gradients*. A very undesirable property of the sigmoid neuron is that when the neuron's activation saturates at either tail of 0 or 1, the gradient at these regions is almost 0. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate's output for the whole objective. Therefore, if the local gradient is very small, it'll effectively "kill" the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. *If the initial weights are too large then most neurons would become saturated and the network will barely learn*.
* *Sigmoid outputs are not zero-centered*. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $x > 0$ elementwise in $f = w^Tx + b$), then the gradient on the weights $w$ will during backpropagation become either all be positive, or all negative. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. The logistic sigmoid function can cause a neural network to get stuck at the training time.

![Sigmoid zig-zagging](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-non-zero-centered-sigmoid.jpg?raw=true)

## Tanh or Hyperbolic Tangent Activation Function

$$tanh(x) = 2{\sigma}(2x) - 1 = \frac{2}{1 + e^{-2x}} - 1$$

Tanh is also sigmoidal (s-shaped). It squashes a real-valued number to the range -1 to 1. Like the sigmoid neuron, its activations saturate, but unlike the sigmoid neuron its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity.

The advantage is that the negative inputs will be mapped strongly negative and the zero inputs will be mapped near zero in the tanh graph.

The function is differentiable and monotonic while its derivative is not monotonic.

**The tanh function is mainly used classification between two classes.**

> Both tanh and logistic sigmoid activation functions are used in feed-forward nets.

## ReLU (Rectified Linear Unit) Activation Function

$$f(x) = max(0,x)$$

The ReLU is the most used activation function in the world right now. It's half rectified (from bottom).

![Sigmoid v.s. ReLU](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-ReLU-vs-sigmoid.png?raw=true)

The function and its derivate both are monotonic.

The pros and cons to use ReLUs:

* (+) It was found to greatly accelerate to the convergence of stochastic gradient descent compared to the simoid/tanh functions. It's argued that this is due to its linear, non-saturating form.
* (+) Compared to tanh/sigmoid neurons that involve expensive operations (exponentials, etc.), the ReLU can be implemented by simply thresholding a matrix of activations at zero.
* (-) ReLUs can be fragile during training and can "die". For example, a large gradient (the learning rate is too high) flowing through a ReLU neuron could cause the weights to update in such a way that the neuron will never activate on any datapoint again. If this happens, then the gradient flowing through the unit will forever be zero from that point on. That is, the ReLU units can irreversibly die during training since they can get knocked off the data manifold. For example, you may find that as much as 40% of your network can be “dead” (i.e. neurons that never activate across the entire training dataset) if the learning rate is set too high. With a proper setting of the learning rate this is less frequently an issue.
* (-) All the negative values become 0 immediately whichi decreases the ability of the model to fit or train from the data properly. That means any negative input given to the ReLU activation function turns the value into 0 immediately in the graph, which in turns affects the resulting graph by not mapping the negative values appropriately.
* (-) Not zero-centered

## Leaky ReLU

$$f(x) = {\alpha}x (x < 0, \alpha is a small constant, e.g. 0.01); f(x) = x (x {\geqslant} 0)$$

Leaky ReLUs are one attempt to fix the "dying ReLU" problem. Usually, the value of $\alpha$ is 0.01 or so. When $\alpha$ is not 0.01 then it is called **Randomized ReLU**. Both leaky and randomized ReLU functions are monotonic. Also, their derivatives also monotonic.

![ReLU v.s. Leaky ReLU](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-Leaky-ReLU.jpg?raw=true)

The consistency of the benefit across tasks is presently unclear.

## Maxout

$$f(x) = max({w^T_1}x + b_1, {w^T_2}x + b_2)$$

Other types of units have been proposed that do not have the functional form f(wTx+b) where a non-linearity is applied on the dot product between the weights and the data. One relatively popular choice is the Maxout neuron that generalizes the ReLU and its leaky version.

Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have $w_1,b_1=0$). The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the number of parameters for every single neuron, leading to a high total number of parameters.

This concludes our discussion of the most common types of neurons and their activation functions. As a last comment, it is very rare to mix and match different types of neurons in the same network, even though there is no fundamental problem with doing so.

## References

* [Activation Functions: Neural Networks](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)
* [Delving Deep into Rectifiers: Surpassing Huamn-Level Performance on ImageNet Classification](https://arxiv.org/pdf/1502.01852.pdf)