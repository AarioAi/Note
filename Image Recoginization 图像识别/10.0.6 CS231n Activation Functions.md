# Activation Functions

It's also known as **Transfer Function**. It's used to determin the output of neural network like yes or no.

## Sigmoid or Logistic Activation Function

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

![Sigmoid and Tanh](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-activation-functions.jpg?raw=true)

The sigmoid function has seen frequent use historically since it has a nice interpretation as the firing rate of a neuron: from not firing at all (0) to fully-saturated firing at an assumed maximum frequency (1). *Since probability of anything exists only between the range 0 and 1, sigmoid is the right choice.* The **softmax function** is a more generalized logistic activation function which is used for multiclass classification.

In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:

* *Sigmoids saturate and kill gradients*. A very undesirable property of the sigmoid neuron is that when the neuron's activation saturates at either tail of 0 or 1, the gradient at these regions is almost 0. Recall that during backpropagation, this (local) gradient will be multiplied to the gradient of this gate's output for the whole objective. Therefore, if the local gradient is very small, it'll effectively "kill" the gradient and almost no signal will flow through the neuron to its weights and recursively to its data. *If the initial weights are too large then most neurons would become saturated and the network will barely learn*.
* *Sigmoid outputs are not zero-centered*. This has implications on the dynamics during gradient descent, because if the data coming into a neuron is always positive (e.g. $x > 0$ elementwise in $f = w^Tx + b$), then the gradient on the weights $w$ will during backpropagation become either all be positive, or all negative. This could introduce undesirable zig-zagging dynamics in the gradient updates for the weights. 

## References

* https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6