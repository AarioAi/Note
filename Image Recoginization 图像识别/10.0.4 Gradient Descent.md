# Gradient Descent


## Gradient Descent and Learning Rate

$$f(x) = kx + b \rightarrow  k = \frac{{\delta}f(x)}{{\delta}x} = lim_{h->0}\frac{f(x+h) - f(x)}{h}$$


$$f(x,y)=xy  \rightarrow  \frac{{\delta}f}{{\delta}x} = y,  \frac{{\delta}f}{{\delta}x} = x$$

$$ gradient: \frac{d(fx)}{dx} = lim_{h->0}\frac{f(x+h)-f(x)}{h}$$



![Gradient Demo](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-gradient-demo.jpg?raw=true)



![Gradient](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-gradient.jpg?raw=true)

* numerical gradient: approximate, slow, easy to write
* analytic gradient: exact, fast, error-prone
  
> **Gradient Check**: in practice,  always use analystic gradient, but check implementation with numerical gradient.
> Only use a small portion of the training set to compute the gradient. Common mini-batch sizes are 32/64/128/256 examples.

```python
while True:
    data_batch = sample_training_data(data, 256)    # sample 256 examples
    weights_grad = evaluate_gradient(loss_fun, data, weights)
    weights += -learning_rate * weight_grad    #  learning_rate is also called step_size
```

![The effects of learning rate](https://github.com/AarioAi/Note/blob/master/Image%20Recoginization%20%E5%9B%BE%E5%83%8F%E8%AF%86%E5%88%AB/_asset/CS231n-the-effects-of-learning-rate.jpg?raw=true)

## Reference
* http://cs231n.github.io/linear-classify/
* http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/